{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring COCO & DETR for Object Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first make sure that a GPU is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep imports and helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import sys\n",
    "sys.path.append('/')\n",
    "sys.path.append('/detr')\n",
    "import os\n",
    "import json\n",
    "from attrdict import AttrDict\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchvision.transforms as T\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "from determined.experimental import Determined\n",
    "from determined.pytorch import PyTorchTrialContext\n",
    "\n",
    "sys.path.append('detr_coco_pytorch')\n",
    "from data import CocoDetection, build_dataset\n",
    "from model import build_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions\n",
    "# COCO classes\n",
    "CLASSES = [\n",
    "    'N/A', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n",
    "    'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A',\n",
    "    'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse',\n",
    "    'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack',\n",
    "    'umbrella', 'N/A', 'N/A', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis',\n",
    "    'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove',\n",
    "    'skateboard', 'surfboard', 'tennis racket', 'bottle', 'N/A', 'wine glass',\n",
    "    'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich',\n",
    "    'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake',\n",
    "    'chair', 'couch', 'potted plant', 'bed', 'N/A', 'dining table', 'N/A',\n",
    "    'N/A', 'toilet', 'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard',\n",
    "    'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N/A',\n",
    "    'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier',\n",
    "    'toothbrush'\n",
    "]\n",
    "\n",
    "# colors for visualization\n",
    "COLORS = [[0.000, 0.447, 0.741], [0.850, 0.325, 0.098], [0.929, 0.694, 0.125],\n",
    "          [0.494, 0.184, 0.556], [0.466, 0.674, 0.188], [0.301, 0.745, 0.933]]\n",
    "\n",
    "# This function will help us plot images and bounding boxes with class labels.\n",
    "def plot_results(pil_img, class_ids, boxes):\n",
    "    plt.figure(figsize=(16,10))\n",
    "    plt.imshow(pil_img)\n",
    "    ax = plt.gca()\n",
    "    colors = COLORS * 100\n",
    "    for cl, (xmin, ymin, xmax, ymax), c in zip(class_ids, boxes.tolist(), colors):\n",
    "        ax.add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin,\n",
    "                                   fill=False, color=c, linewidth=3))\n",
    "        text = f'{CLASSES[cl]}'\n",
    "        ax.text(xmin, ymin, text, fontsize=15,\n",
    "                bbox=dict(facecolor='yellow', alpha=0.5))\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Standard transform\n",
    "transform = T.Compose([\n",
    "    T.Resize(800),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "def box_cxcywh_to_xyxy(x):\n",
    "    x_c, y_c, w, h = x.unbind(1)\n",
    "    b = [(x_c - 0.5 * w), (y_c - 0.5 * h),\n",
    "         (x_c + 0.5 * w), (y_c + 0.5 * h)]\n",
    "    return torch.stack(b, dim=1)\n",
    "\n",
    "def rescale_bboxes(out_bbox, size):\n",
    "    img_w, img_h = size\n",
    "    b = box_cxcywh_to_xyxy(out_bbox)\n",
    "    b = b * torch.tensor([img_w, img_h, img_w, img_h], dtype=torch.float32)\n",
    "    return b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COCO Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be working with the COCO 2017 dataset so let's first get familiar with what the data looks like.  Since the notebook you're using is launched and managed from the Determined cluster, it has already been configured to have access to a google storage bucket with COCO 2017 data.  We can just point the dataset to the right bucket to load image files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_val = CocoDetection(\n",
    "    'gcs',\n",
    "    'determined-ai-coco-dataset',\n",
    "    'determined-ai-coco-dataset/val2017',\n",
    "    '/tmp/instances_val2017.json',\n",
    "    transforms=None,\n",
    "    return_masks=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The COCO 2017 dataset has around 90 classes along with hierarchical grouping of classes by super category.  We will be tasked with predicting bounding boxes and classes for the granular classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_val.coco.cats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at one of the images along with the associated target bounding boxes and classes.  As you can see, boxes can overlap and there can be multiple instances of the same object in one image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapoint = dataset_val[237]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(datapoint[0], datapoint[1]['labels'], datapoint[1]['boxes'])    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview of DETR architecture (from FAIR's [DETR Demo](https://colab.research.google.com/github/facebookresearch/detr/blob/colab/notebooks/detr_demo.ipynb#scrollTo=h91rsIPl7tVl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](detr_coco_pytorch/imgs/detr_architecture.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DETRdemo(nn.Module):\n",
    "    \"\"\"\n",
    "    Demo DETR implementation.\n",
    "\n",
    "    Demo implementation of DETR in minimal number of lines, with the\n",
    "    following differences wrt DETR in the paper:\n",
    "    * learned positional encoding (instead of sine)\n",
    "    * positional encoding is passed at input (instead of attention)\n",
    "    * fc bbox predictor (instead of MLP)\n",
    "    The model achieves ~40 AP on COCO val5k and runs at ~28 FPS on Tesla V100.\n",
    "    Only batch size 1 supported.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes, hidden_dim=256, nheads=8,\n",
    "                 num_encoder_layers=6, num_decoder_layers=6):\n",
    "        super().__init__()\n",
    "\n",
    "        # create ResNet-50 backbone\n",
    "        self.backbone = resnet50()\n",
    "        del self.backbone.fc\n",
    "\n",
    "        # create conversion layer\n",
    "        self.conv = nn.Conv2d(2048, hidden_dim, 1) \n",
    "\n",
    "        # create a default PyTorch transformer\n",
    "        self.transformer = nn.Transformer(\n",
    "            hidden_dim, nheads, num_encoder_layers, num_decoder_layers)\n",
    "\n",
    "        # prediction heads, one extra class for predicting non-empty slots\n",
    "        # note that in baseline DETR linear_bbox layer is 3-layer MLP\n",
    "        self.linear_class = nn.Linear(hidden_dim, num_classes + 1)\n",
    "        self.linear_bbox = nn.Linear(hidden_dim, 4)\n",
    "\n",
    "        # output positional encodings (object queries)\n",
    "        self.query_pos = nn.Parameter(torch.rand(100, hidden_dim))\n",
    "\n",
    "        # spatial positional encodings\n",
    "        # note that in baseline DETR we use sine positional encodings\n",
    "        self.row_embed = nn.Parameter(torch.rand(50, hidden_dim // 2))\n",
    "        self.col_embed = nn.Parameter(torch.rand(50, hidden_dim // 2))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # propagate inputs through ResNet-50 up to avg-pool layer\n",
    "        x = self.backbone.conv1(inputs)\n",
    "        x = self.backbone.bn1(x)\n",
    "        x = self.backbone.relu(x)\n",
    "        x = self.backbone.maxpool(x)\n",
    "\n",
    "        x = self.backbone.layer1(x)\n",
    "        x = self.backbone.layer2(x)\n",
    "        x = self.backbone.layer3(x)\n",
    "        x = self.backbone.layer4(x)\n",
    "\n",
    "        # convert from 2048 to 256 feature planes for the transformer\n",
    "        h = self.conv(x) # output will be 256 x h x w\n",
    "\n",
    "        # construct positional encodings\n",
    "        H, W = h.shape[-2:]\n",
    "        pos = torch.cat([\n",
    "            self.col_embed[:W].unsqueeze(0).repeat(H, 1, 1),\n",
    "            self.row_embed[:H].unsqueeze(1).repeat(1, W, 1),\n",
    "        ], dim=-1).flatten(0, 1).unsqueeze(1) # hw x 1 x 256\n",
    "\n",
    "        # propagate through the transformer\n",
    "        # add positional encoding to cnn features\n",
    "        tr_enc_input = pos + 0.1 * h.flatten(2).permute(2, 0, 1) # hw x batch_size x 256\n",
    "        # Get 100 query corresponding to 100 proposals ready to input into deoder\n",
    "        tr_dec_input = self.query_pos.unsqueeze(1) # 100 x 1 x 256\n",
    "        h = self.transformer(tr_enc_input, tr_dec_input).transpose(0, 1) # batch_size x 100 x 256\n",
    "                             \n",
    "        \n",
    "        # finally project transformer outputs to class labels and bounding boxes\n",
    "        return {'pred_logits': self.linear_class(h), \n",
    "                'pred_boxes': self.linear_bbox(h).sigmoid()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DETR in Determined\n",
    "from model_def import DETRTrial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "??DETRTrial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submit experiment to Determined Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the trial definition is ready to go, we can configure an experiment with the desired hyperparameters and resources with a `.yaml` config file.  This separation between config and code promotes reproducibility down the road.  We also track the environment and code used for each experiment so that users can easily replicate their results in the future.  \n",
    "\n",
    "The magic of Determined is that users need to change very few things to run advanced ML workflows such as multi-node distributed training and advanced hyperparameter search.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat detr_coco_pytorch/finetune_gcs.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You can test this out by running `det e create finetune_gcs.yaml .` from the `detr_coco_pytorch` folder using the terminal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading models from registry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After an experiment finishes running, we can track the resulting checkpoints if desired in the model registry.  This can be done through the command line (see below) or programmatically with python.  \n",
    "\n",
    "`det model create <model_name>`\n",
    "\n",
    "`det model register-version <model_name> <checkpoint_uuid>`\n",
    "\n",
    "Once a checkpoint has been registered under a model, it can be easily accessed for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Determined().get_model(\"detr\")\n",
    "ckpt = model.get_version()\n",
    "ckpt_path = ckpt.download()\n",
    "ckpt = torch.load(os.path.join(ckpt_path, 'state_dict.pth'))\n",
    "metadata = json.load(open(os.path.join(ckpt_path, 'metadata.json'), 'r'))\n",
    "hparams = AttrDict(metadata['hparams'])\n",
    "model, _, postprocessor = build_model(hparams, 1)\n",
    "model.load_state_dict(ckpt['models_state_dict'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "raw_img = dataset_val[772][0]\n",
    "img = transform(raw_img).unsqueeze(0)\n",
    "outputs = model(img)\n",
    "\n",
    "# keep only predictions with 0.9+ confidence\n",
    "probas = outputs['pred_logits'].softmax(-1)[0, :, :-1]\n",
    "keep = probas.max(-1).values > 0.9\n",
    "\n",
    "bboxes_scaled = rescale_bboxes(outputs['pred_boxes'][0, keep], raw_img.size)\n",
    "plot_results(raw_img, probas[keep].argmax(-1), bboxes_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing decoder attention weights (modified from [DETR notebook](https://colab.research.google.com/github/facebookresearch/detr/blob/colab/notebooks/detr_attention.ipynb#scrollTo=PcxWAOzOYTEn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_decoder_weights(model, raw_img, img):\n",
    "    conv_features, dec_attn_weights = [], []\n",
    "\n",
    "    hooks = [\n",
    "        model.backbone[-2].register_forward_hook(\n",
    "            lambda self, input, output: conv_features.append(output)\n",
    "        ),\n",
    "        model.transformer.decoder.layers[-1].multihead_attn.register_forward_hook(\n",
    "            lambda self, input, output: dec_attn_weights.append(output[1])\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    # propagate through the model\n",
    "    outputs = model(img)\n",
    "\n",
    "    for hook in hooks:\n",
    "        hook.remove()\n",
    "\n",
    "    # don't need the list anymore\n",
    "    conv_features = conv_features[0]\n",
    "    dec_attn_weights = dec_attn_weights[0]\n",
    "    \n",
    "    # get the feature map shape\n",
    "    h, w = conv_features['0'].tensors.shape[-2:]\n",
    "\n",
    "    fig, axs = plt.subplots(ncols=len(bboxes_scaled), nrows=2, figsize=(22, 7))\n",
    "    colors = COLORS * 100\n",
    "    for idx, ax_i, (xmin, ymin, xmax, ymax) in zip(keep.nonzero(), axs.T, bboxes_scaled):\n",
    "        ax = ax_i[0]\n",
    "        ax.imshow(dec_attn_weights[0, idx].view(h, w).detach())\n",
    "        ax.axis('off')\n",
    "        ax.set_title(f'query id: {idx.item()}')\n",
    "        ax = ax_i[1]\n",
    "        ax.imshow(raw_img)\n",
    "        ax.add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin,\n",
    "                                   fill=False, color='blue', linewidth=3))\n",
    "        ax.axis('off')\n",
    "        ax.set_title(CLASSES[probas[idx].argmax()])\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_decoder_weights(model, raw_img, img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure HP search experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To tune hyperparameters of our finetune experiment, we simply need to modify a few fields in the experiment config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat detr_coco_pytorch/finetune_adaptive.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You can test this out by running `det e create finetune_adaptive.yaml .` from the `detr_coco_pytorch` folder using the terminal."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
