{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "513c47aa-5c42-4f78-9bf9-c1fcee73eaf2",
   "metadata": {},
   "source": [
    "# Building Conversational AI with Transformers and Determined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "306b77ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import random\n",
    "import json\n",
    "import warnings\n",
    "\n",
    "from itertools import chain\n",
    "from pprint import pformat\n",
    "from attrdict import AttrDict\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import OpenAIGPTLMHeadModel, OpenAIGPTTokenizerFast, GPT2LMHeadModel, GPT2TokenizerFast, cached_path\n",
    "from data import SPECIAL_TOKENS, build_input_from_segments, add_special_tokens_\n",
    "from utils import get_dataset, download_pretrained_model\n",
    "from example_input import build_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e758f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arguments\n",
    "args = AttrDict()\n",
    "args.dataset_path = \"\"\n",
    "args.dataset_cache = \"/root/.cache/\"\n",
    "args.model = \"openai-gpt\"\n",
    "args.ckpt_uuid = \"24f33c0f-d5fd-4cc3-8551-db48f32f8fc2\"\n",
    "args.max_history = 5\n",
    "args.no_sample = False\n",
    "args.max_length = 40\n",
    "args.min_length = 1\n",
    "args.temperature = 0.7\n",
    "args.top_k = 0\n",
    "args.top_p = 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884b08db-e80e-477c-9a05-60af0a0061f7",
   "metadata": {},
   "source": [
    "## Let's examine the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af2b1116-64b1-4871-876d-1c82506a486f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download dataset from https://s3.amazonaws.com/datasets.huggingface.co/personachat/personachat_self_original.json\n"
     ]
    }
   ],
   "source": [
    "PERSONACHAT_URL = \"https://s3.amazonaws.com/datasets.huggingface.co/personachat/personachat_self_original.json\"\n",
    "print(\"Download dataset from {}\".format(PERSONACHAT_URL))\n",
    "personachat_file = cached_path(PERSONACHAT_URL)\n",
    "with open(personachat_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    dataset = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98f06296-3b30-458f-a4ee-3e6197c8012c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are a total of 17878 dialogues in the training set\n"
     ]
    }
   ],
   "source": [
    "print(\"There are a total of {} dialogues in the training set\".format(len(dataset[\"train\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e17143c1-8728-4747-bc79-f4420fcd5231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Personality:  ['i love to sing .', 'i am a night owl .', \"i'm a dancer .\", 'i can play the piano .', \"i'm a vegetarian .\"]\n",
      "Utterance:  {'candidates': ['that is so fun boys are awesome', 'hospitals are lame , you should make a run for it .', 'wow just finished reading ender s game , what a great book !', 'it is ok , we do other things like go to the park or zoo', 'what do police do for fun ? who says you gonna learn today .', 'hello there ! what are your hobbies ?', 'i am good ! waiting for my wife to get home .', 'whats your favorite color ? mine is purple .', 'how long have you been friends', 'i am not a big foodie . i prefer crafts , like whittling .', 'oh , ok sure . anything else ?', \"i guess i'll make you a salad p\", 'hi , how are you today', 'no siblings myself . are you at work ?', 'i bet . what do you do for fun ?', 'at this joint called the frog zone grill . its pretty chill . what about you ?', 'is the cat hairless ?', \"www . cafepress . com lelesfashionshop1 is the link to my shop and yes they're\", 'cool . what sports did you play ?', \"hi ! i've just been sitting here playing the piano and singing along\"], 'history': ['hi i am sally , i live with my sweet dogs in taos , new mexico .']}\n"
     ]
    }
   ],
   "source": [
    "# Example\n",
    "obs_ind = 10\n",
    "sample = dataset[\"train\"][obs_ind]\n",
    "persona = sample[\"personality\"]\n",
    "history = sample[\"utterances\"][0][\"history\"]\n",
    "reply = sample[\"utterances\"][0][\"candidates\"][-1][:-1]\n",
    "print(\"Personality: \", persona)\n",
    "print(\"Utterance: \", sample[\"utterances\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6678e69d-79f5-476e-95a1-88dfccd80032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words: ['<bos>', 'i', 'love', 'to', 'sing', '.', 'i', 'am', 'a', 'night', 'owl', '.', \"i'm\", 'a', 'dancer', '.', 'i', 'can', 'play', 'the', 'piano', '.', \"i'm\", 'a', 'vegetarian', '.', '<speaker2>', 'hi', 'i', 'am', 'sally', ',', 'i', 'live', 'with', 'my', 'sweet', 'dogs', 'in', 'taos', ',', 'new', 'mexico', '.', '<speaker1>', 'hi', '!', \"i've\", 'just', 'been', 'sitting', 'here', 'playing', 'the', 'piano', 'and', 'singing', 'alon', '<eos>']\n",
      "\n",
      "Segments: ['<speaker1>', '<speaker1>', '<speaker1>', '<speaker1>', '<speaker1>', '<speaker1>', '<speaker1>', '<speaker1>', '<speaker1>', '<speaker1>', '<speaker1>', '<speaker1>', '<speaker1>', '<speaker1>', '<speaker1>', '<speaker1>', '<speaker1>', '<speaker1>', '<speaker1>', '<speaker1>', '<speaker1>', '<speaker1>', '<speaker1>', '<speaker1>', '<speaker1>', '<speaker1>', '<speaker2>', '<speaker2>', '<speaker2>', '<speaker2>', '<speaker2>', '<speaker2>', '<speaker2>', '<speaker2>', '<speaker2>', '<speaker2>', '<speaker2>', '<speaker2>', '<speaker2>', '<speaker2>', '<speaker2>', '<speaker2>', '<speaker2>', '<speaker2>', '<speaker1>', '<speaker1>', '<speaker1>', '<speaker1>', '<speaker1>', '<speaker1>', '<speaker1>', '<speaker1>', '<speaker1>', '<speaker1>', '<speaker1>', '<speaker1>', '<speaker1>', '<speaker1>', '<speaker1>']\n",
      "\n",
      "Position: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Input example\n",
    "words, segments, position, sequence = build_inputs(\n",
    "    [p.split(' ') for p in persona], \n",
    "    [h.split(' ') for h in history], \n",
    "    reply.split(' ')\n",
    ")\n",
    "print(\"Words: {}\\n\".format(words))\n",
    "print(\"Segments: {}\\n\".format(segments))\n",
    "print(\"Position: {}\\n\".format(position))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2505ab-a779-40dc-971f-46f003620edf",
   "metadata": {},
   "source": [
    "## Interact with a trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259f42f6-0a84-42a3-856e-e54aebbc08d1",
   "metadata": {},
   "source": [
    "### Load dataset and pretrained model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ccb6d82-3193-4b77-9beb-578a1388303b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get pretrained tokenizer and dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of OpenAIGPTLMHeadModel were not initialized from the model checkpoint at openai-gpt and are newly initialized: ['lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "print(\"Get pretrained tokenizer and dataset\")\n",
    "tokenizer_class, model_class = (GPT2TokenizerFast, GPT2LMHeadModel) if args.model == 'gpt2' else (OpenAIGPTTokenizerFast, OpenAIGPTLMHeadModel)\n",
    "tokenizer = tokenizer_class.from_pretrained(args.model)\n",
    "model = model_class.from_pretrained(args.model)\n",
    "add_special_tokens_(model, tokenizer)\n",
    "dataset = get_dataset(tokenizer, args.dataset_path, args.dataset_cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ad97552-80aa-45f3-9d32-97119b3fad02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load pretrained model from Determined checkpoint\n"
     ]
    }
   ],
   "source": [
    "print(\"Load pretrained model from Determined checkpoint\")\n",
    "from determined.experimental import Determined\n",
    "ckpt_path = Determined().get_checkpoint(args.ckpt_uuid).download()\n",
    "ckpt = torch.load(ckpt_path + \"/state_dict.pth\")\n",
    "model.load_state_dict(ckpt['models_state_dict'][0], strict=False)\n",
    "model = model.cuda()\n",
    "## Another way of loading a checkpoint from Determined that loads the actual Trial.  \n",
    "## This is slower because it performs the full init for the trial, including tokenizing the dataset.\n",
    "#ckpt = Determined().get_checkpoint(args.ckpt_uuid).load()\n",
    "#model = ckpt.model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0642a6c-cd30-47bd-92c2-c4b96856fc7c",
   "metadata": {},
   "source": [
    "### Sample a personality and interact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "280c68f1-b794-416a-abc0-0674bef36420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for interacting with trained model\n",
    "def top_filtering(logits, top_k=0., top_p=0.9, threshold=-float('Inf'), filter_value=-float('Inf')):\n",
    "    \"\"\" Filter a distribution of logits using top-k, top-p (nucleus) and/or threshold filtering\n",
    "        Args:\n",
    "            logits: logits distribution shape (vocabulary size)\n",
    "            top_k: <=0: no filtering, >0: keep only top k tokens with highest probability.\n",
    "            top_p: <=0.0: no filtering, >0.0: keep only a subset S of candidates, where S is the smallest subset\n",
    "                whose total probability mass is greater than or equal to the threshold top_p.\n",
    "                In practice, we select the highest probability tokens whose cumulative probability mass exceeds\n",
    "                the threshold top_p.\n",
    "            threshold: a minimal threshold to keep logits\n",
    "    \"\"\"\n",
    "    assert logits.dim() == 1  # Only work for batch size 1 for now - could update but it would obfuscate a bit the code\n",
    "    top_k = min(top_k, logits.size(-1))\n",
    "    if top_k > 0:\n",
    "        # Remove all tokens with a probability less than the last token in the top-k tokens\n",
    "        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
    "        logits[indices_to_remove] = filter_value\n",
    "\n",
    "    if top_p > 0.0:\n",
    "        # Compute cumulative probabilities of sorted tokens\n",
    "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "        cumulative_probabilities = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "\n",
    "        # Remove tokens with cumulative probability above the threshold\n",
    "        sorted_indices_to_remove = cumulative_probabilities > top_p\n",
    "        # Shift the indices to the right to keep also the first token above the threshold\n",
    "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "        sorted_indices_to_remove[..., 0] = 0\n",
    "\n",
    "        # Back to unsorted indices and set them to -infinity\n",
    "        indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "        logits[indices_to_remove] = filter_value\n",
    "\n",
    "    indices_to_remove = logits < threshold\n",
    "    logits[indices_to_remove] = filter_value\n",
    "\n",
    "    return logits\n",
    "\n",
    "\n",
    "def sample_sequence(personality, history, tokenizer, model, args, current_output=None):\n",
    "    special_tokens_ids = tokenizer.convert_tokens_to_ids(SPECIAL_TOKENS)\n",
    "    if current_output is None:\n",
    "        current_output = []\n",
    "\n",
    "    for i in range(args.max_length):\n",
    "        instance = build_input_from_segments(personality, history, current_output, tokenizer, with_eos=False)\n",
    "\n",
    "        input_ids = torch.tensor(instance[\"input_ids\"], device=\"cuda\").unsqueeze(0)\n",
    "        token_type_ids = torch.tensor(instance[\"token_type_ids\"], device=\"cuda\").unsqueeze(0)\n",
    "\n",
    "        outputs = model(input_ids, token_type_ids=token_type_ids)\n",
    "        logits = outputs.logits\n",
    "        if isinstance(logits, tuple):  # for gpt2 and maybe others\n",
    "            logits = logits[0]\n",
    "        logits = logits[0, -1, :] / args.temperature\n",
    "        logits = top_filtering(logits, top_k=args.top_k, top_p=args.top_p)\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "        prev = torch.topk(probs, 1)[1] if args.no_sample else torch.multinomial(probs, 1)\n",
    "        if i < args.min_length and prev.item() in special_tokens_ids:\n",
    "            while prev.item() in special_tokens_ids:\n",
    "                if probs.max().item() == 1:\n",
    "                    warnings.warn(\"Warning: model generating special token with probability 1.\")\n",
    "                    break  # avoid infinitely looping over special token\n",
    "                prev = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "        if prev.item() in special_tokens_ids:\n",
    "            break\n",
    "        current_output.append(prev.item())\n",
    "\n",
    "    return current_output\n",
    "\n",
    "def sample_personality(dataset, no_personality=False):\n",
    "    if no_personality:\n",
    "        return []    \n",
    "    personalities = [dialog[\"personality\"] for dataset in dataset.values() for dialog in dataset]\n",
    "    personality = random.choice(personalities)\n",
    "    print(\"Selected personality is: {}\".format(' '.join(tokenizer.batch_decode(personality))))\n",
    "    return personality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "71fd1944-2405-4c6d-8165-b4738624b232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling profiles control the behavior of the sequence generation for the response.  \n",
    "# \"low\", \"medium\", and \"high\" roughly correspond to coherence with \n",
    "# \"low\" generating more random response and \"high\" generating very similar responses to the history.\n",
    "sampling_profiles = {\n",
    "    'low': {'top_k': 180, 'top_p': 0.1, 'temperature': 1.9},\n",
    "    'medium': {'top_k': 70, 'top_p': 0.5, 'temperature': 1.2},\n",
    "    'high': {'top_k': 0, 'top_p': 0.9, 'temperature': 0.6},\n",
    "    'custom': {'top_k': 1, 'top_p': 0.7, 'temperature': 1},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "13f98d54-073a-4852-b281-94a406bbab20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected personality is: i've perfect pitch. i've been published in the new yorker magazine. i'm a gourmet cook. as a child, i won a national spelling bee.\n"
     ]
    }
   ],
   "source": [
    "profile = 'medium'\n",
    "for v in ['top_k', 'top_p', 'temperature']:\n",
    "    args[v] = sampling_profiles[profile][v]\n",
    "personality = sample_personality()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ddc640",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = []\n",
    "while True:\n",
    "    raw_text = input(\">>> \")\n",
    "    while not raw_text:\n",
    "        print('Prompt should not be empty!')\n",
    "        raw_text = input(\">>> \")\n",
    "    history.append(tokenizer.encode(raw_text))\n",
    "    with torch.no_grad():\n",
    "        out_ids = sample_sequence(personality, history, tokenizer, model, args)\n",
    "    history.append(out_ids)\n",
    "    history = history[-(2*args.max_history+1):]\n",
    "    out_text = tokenizer.decode(out_ids, skip_special_tokens=True)\n",
    "    print(out_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
